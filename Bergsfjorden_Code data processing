#structure
#3.2.1 ADCP data processing
## Quality control (Correlation, cross difference)
## Accounting for loss and movement (Removing vertical movement, time varied gain)

#3.2.2 CTD data processing
##Computation density
##Data alignment
#3.2.2 Sediment trap data processing 
#3.2.2 Meteorological data processing
##Data alignment
##Computation solar irradiance
#3.2.2 Aquaculture data processing
##Data alignment
##Combining net cages

#3.2.3 Time series analysis
##Interpolation
##FFT
##Low-pass filters
##Band-pass filters
##Model days

#EXPORT


#import packages
import scipy.io
import scipy.interpolate
import math
import scipy.signal as sig
import scipy.fftpack as sf
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from matplotlib.colors import ListedColormap, BoundaryNorm
from matplotlib import cm
import datetime
import netCDF4
from math import pi
import gsw
import pickle
import netCDF4 as nc
from netCDF4 import num2date, date2num
from scipy.signal import welch, hanning
import seaborn
import mat73


#Section 3.2.1 in the thesis - ADCP data processing - _________________________________________________________________________________________________________________________

#Import ADCP data Bergsfjorden_MATLAB dataset as dictionary
mat= scipy.io.loadmat('Bergsfjorden_data_from Excel.mat', squeeze_me=True,struct_as_record=False)
matCTD= mat73.loadmat('Bergsfjorden_CTD_NORFJORD.mat')

#General
Depth=mat['data'].Depth[3:7960]                 #m
Pressure=mat['data'].Pressure[3:7960]
Ping=mat['data'].PingCount[3:7960]              #No deviations
Time=pd.to_datetime((mat['data'].time[3:7960])-719529, unit='D').round('s')
Binsize=2 #m
Blankingdistance=1.5 #m
Watercolumn=np.arange(0,np.max(Depth),1)
RangeADCP=np.arange(0,np.max(Depth),1)

#Backscatter + vertical velocity
Backscatsurf=mat['data'].Strength[0,3:7960]                             #dB
Backscatcol1=mat['data'].Strength[1:45,3:7960]                          #dB
Backscatcol2=mat['data'].Strength[51:91,3:7960]                         #dB
Vertvelsurf=mat['data'].VerticalSpeed[0,3:7960]                         #dB
Vertvelcol1=mat['data'].VerticalSpeed[1:45,3:7960]                      #cm/s
Vertvelcol2=mat['data'].VerticalSpeed[51:91,3:7960]                     #cm/s
VertvelABsurf=mat['data'].VerticalAutoBeam[0,3:7960]                    #cm/s
VertvelABcol1=mat['data'].VerticalAutoBeam[1:45,3:7960]                 #cm/s
VertvelABcol2=mat['data'].VerticalAutoBeam[51:91,3:7960]                #cm/s
Correlationbeam1surf=mat['data'].Beam1CorrelationFactor[0,3:7960]       #cm/s only for speed
Correlationbeam2surf=mat['data'].Beam2CorrelationFactor[0,3:7960]       #cm/s only for speed
Correlationbeam3surf=mat['data'].Beam3CorrelationFactor[0,3:7960]       #cm/s only for speed
Correlationbeam4surf=mat['data'].Beam4CorrelationFactor[0,3:7960]       #cm/s only for speed
Correlationbeam1col1=mat['data'].Beam1CorrelationFactor[1:45,3:7960]    #cm/s only for speed
Correlationbeam2col1=mat['data'].Beam2CorrelationFactor[1:45,3:7960]    #cm/s only for speed
Correlationbeam3col1=mat['data'].Beam3CorrelationFactor[1:45,3:7960]    #cm/s only for speed
Correlationbeam4col1=mat['data'].Beam4CorrelationFactor[1:45,3:7960]    #cm/s only for speed
Correlationbeam1col2=mat['data'].Beam1CorrelationFactor[51:91,3:7960]   #cm/s only for speed
Correlationbeam2col2=mat['data'].Beam2CorrelationFactor[51:91,3:7960]   #cm/s only for speed
Correlationbeam3col2=mat['data'].Beam3CorrelationFactor[51:91,3:7960]   #cm/s only for speed
Correlationbeam4col2=mat['data'].Beam4CorrelationFactor[51:91,3:7960]   #cm/s only for speed
Crossdifferencesurf=mat['data'].CrossDifference[0,3:7960]               #cm/s Should be close to zero
Crossdifferencecol1=mat['data'].CrossDifference[1:45,3:7960]            #cm/s Should be close to zero
Crossdifferencecol2=mat['data'].CrossDifference[51:91,3:7960]           #cm/s Should be close to zero
Noisepeakbeam1=mat['data'].NoisePeakLevelB1[3:7960] 	                  #dB
Noisepeakbeam2=mat['data'].NoisePeakLevelB2[3:7960] 	                  #dB
Noisepeakbeam3=mat['data'].NoisePeakLevelB3[3:7960] 	                  #dB
Noisepeakbeam4=mat['data'].NoisePeakLevelB4[3:7960] 	                  #dB
Stdevbeam1surf=mat['data'].Beam1Stdev[0,3:7960]
Stdevbeam2surf=mat['data'].Beam2Stdev[0,3:7960]
Stdevbeam3surf=mat['data'].Beam3Stdev[0,3:7960]
Stdevbeam4surf=mat['data'].Beam4Stdev[0,3:7960]
Stdevbeam1col1=mat['data'].Beam1Stdev[1:51,3:7960]
Stdevbeam2col1=mat['data'].Beam2Stdev[1:51,3:7960]
Stdevbeam3col1=mat['data'].Beam3Stdev[1:51,3:7960]
Stdevbeam4col1=mat['data'].Beam4Stdev[1:51,3:7960]
Stdevbeam1col2=mat['data'].Beam1Stdev[51:91,3:7960]
Stdevbeam2col2=mat['data'].Beam2Stdev[51:91,3:7960]
Stdevbeam3col2=mat['data'].Beam3Stdev[51:91,3:7960]
Stdevbeam4col2=mat['data'].Beam4Stdev[51:91,3:7960]
Speedeast=mat['data'].EastSpeed[1:45,3:7960]
Speednorth=mat['data'].NorthSpeed[1:45,3:7960]
Salinity=mat['data'].Salinity[3:7960]

#Other sensors
Tur=mat['data'].Turbidity14570[3:7960]
Chl=mat['data'].Chlorophyll2103754[3:7960]
O2=mat['data'].O2Concentration[3:7960]
Sal=mat['data'].Salinity[3:7960]                    #PSU
Temp=mat['data'].Temperature1[3:7960]
Den=mat['data'].Density[3:7960]                     #kg/m3
Speedsound=mat['data'].Soundspeed[3:7960]           #m/s
Speedofsound=mat['data'].SpeedOfSound[3:7960]
Tide=mat['data'].TideLevel[3:7960]
Tidepres=mat['data'].TidePressure[3:7960]
Airsaturation=mat['data'].AirSaturation[3:7960]
Conductivity=mat['data'].Conductivity[3:7960]

#Setup ADCP
#surf= 2m from the surface
#col1=50bins of 2m.    Water is around 95.6m deep (100+1.5-95.6)/2= 2.95 ->delete upper three bins.
##Range away from ADCP = (1.5,3.5,5.5,7.5, etc...)
#col2=40bins of 0.5m, with 50% overlap. So 40*0.5*0.50%= 10m. +1.5m start away from ADCP = 11.5m RANGE.
##Range away from ADCP = (1.5,1.75,2.00,2.25,2.50,2.75, etc...)

#Derive depth of all bins from depth ADCP
Binscol1=np.arange(Blankingdistance+len(Backscatcol1)*Binsize,Blankingdistance,Binsize*-1)         #depth of bins
Depthbincol1=np.zeros((len(Binscol1),len(Time)),dtype='float')

for i in range(len(Time)):
    
    Depthbincol1[:,i]=np.arange((Depth[i]-Blankingdistance),(Depth[i]-len(Binscol1)*Binsize-Blankingdistance),Binsize*-1)
    
    i=i+1
               
#Section 3.2.1 in the thesis - Data processing ADCP - Quality control - Correlation and cross difference__________________________________________________________________________ 
##Col1
for i in range(len(Time)):          #per unit of time
    
    for j in range(len(Binscol1)):                #each bin depth
        
    ##CORRELATION        
        if Correlationbeam1col1[j,i] <=0.20:            #If correlation is lower than 20 -> np.NaN
            Backscatcol1[j,i]=np.nan
            VertvelABcol1[j,i]=np.nan
            Vertvelcol1[j,i]=np.nan
        
        if Correlationbeam2col1[j,i] <=0.20:
            Backscatcol1[j,i]=np.nan
            VertvelABcol1[j,i]=np.nan
            Vertvelcol1[j,i]=np.nan    
            
        if Correlationbeam3col1[j,i] <=0.20:
            Backscatcol1[j,i]=np.nan
            VertvelABcol1[j,i]=np.nan
            Vertvelcol1[j,i]=np.nan  
            
        if Correlationbeam4col1[j,i] <=0.20:
            Backscatcol1[j,i]=np.nan
            VertvelABcol1[j,i]=np.nan
            Vertvelcol1[j,i]=np.nan    
            
# =============================================================================
#    ##CROSS DIFFERENCE                 
#         if Crossdifferencecol1[j,i] <=:
#             Backscatcol1[j,i]=np.nan
#             VertvelABcol1[j,i]=np.nan
#             Vertvelcol1[j,i]=np.nan
#             
#     #St dev? and Noise Peak?
#                    
# =============================================================================
        j=j+1
        
    i=i+1
    


#Section 3.2.1 in the thesis - Data processing ADCP - Accounting for loss and movement - Taking movement out of the ADCP____________________________________
Backscatcol1_1=np.zeros((len(Time),int(np.max(Depth))+1),dtype='float')
VertvelABcol1_1=np.zeros((len(Time),int(np.max(Depth))+1),dtype='float')
Vertvelcol1_1=np.zeros((len(Time),int(np.max(Depth))+1),dtype='float')
SpeedEWcol1_1=np.zeros((len(Time),int(np.max(Depth))+1),dtype='float')
SpeedNScol1_1=np.zeros((len(Time),int(np.max(Depth))+1),dtype='float')

#Interpolate from depths that were registered during deployment (and therefore included tides) to m- water depth. -> filters out tides.
for i in range(len(Time)):  #for each measurement
    
    a=scipy.interpolate.interp1d(Depthbincol1[:,i],Backscatcol1[:,i],'linear',fill_value='extrapolate') #interpolate between depth and backscatter/ vertical velocity -> assign to new depth.
    Backscatcol1_1[i,:]=a(Watercolumn)

    a=scipy.interpolate.interp1d(Depthbincol1[:,i],VertvelABcol1[:,i],'linear',fill_value='extrapolate')
    VertvelABcol1_1[i,:]=a(Watercolumn)

    a=scipy.interpolate.interp1d(Depthbincol1[:,i],Vertvelcol1[:,i],'linear',fill_value='extrapolate')
    Vertvelcol1_1[i,:]=a(Watercolumn)

    a=scipy.interpolate.interp1d(Depthbincol1[:,i],Speedeast[:,i],'linear',fill_value='extrapolate')
    SpeedEWcol1_1[i,:]=a(Watercolumn)
    
    a=scipy.interpolate.interp1d(Depthbincol1[:,i],Speednorth[:,i],'linear',fill_value='extrapolate')
    SpeedNScol1_1[i,:]=a(Watercolumn)

    i=i+1
    
#Section 3.2.1 in the thesis - Data processing ADCP - Accounting for loss and movement - Time varied gain DETERMINING DEGREDATION OF SIGNAL OVER DISTANCE FROM AND TOWARDS ADCPs_____________________________    
#f(ADCP)= 250Hz...The Francois & Garrison, 1982 formula is used because it is suitable for a sound frequency 
#range of 200 Hz to 1MH. In seawater sound is absorbed by 3 parts, together forming the total sound absorption
#Part I (boric acid), part II (MgSO4), part III (Pure water)

#For this the following parameters for both Mooring 1 and 2
pH= 7   #Acidity unit: scale 1(acid) - 14(base) 
f=600 #Frequency sound signal ADCP, unit: kHz

CTD_Sal=np.meshgrid(Sal,Watercolumn ) #CTD_SAL 0 are all the salinity values
CTD_Temp=np.meshgrid(Temp,Watercolumn) #CTD_TEMP 0 are all the temperature values
a=np.min(Time)         #First measurement of ADCP 
b=np.max(Time)         #Last measurement of ADCP 
c=np.where(Mtime_2==a)       #Row of CTD measurement where ADCP made its FIRST measurement
d=np.where(Mtime_2==b)       #Row of CTD measurement where ADCP made its LAST measurement

#CHECK
#print(M1timeofmeasurementp[0])
#print(CTD_M1_time_hours[c])
#print(M1timeofmeasurementp[7486])
#print(CTD_M1_time_hours[d])

MSal_rn=MSal_2[:,int(c[0]-1):int(d[0]+1)]    #Make new file where Salinity and temperature are extracted based on ADCP measurments -> result = sal+temp for each ADCP measurement of the ADCP
MTemp_rn=MTemp_2[:,int(c[0]-1):int(d[0]+1)]
MDens_rn=MDens_2[:,int(c[0]-1):int(d[0]+1)]

##Make data file & run loop
Sound_abs=np.zeros((len(Time),len(Watercolumn)),dtype='float')           
Tvg=np.zeros((len(Time),len(Watercolumn)),dtype='float')               
Backscatcol1_2=np.zeros((len(Time),len(Watercolumn)),dtype='float')

for i in range(len(Time)):  #Within that loop do for each week
   
    for j in range(len(Watercolumn)):      #For each depth do the following loop
    
        #Part I: Sound absorption by boric acid CHECK
        c = 1412 + 3.21*MTemp_rn[j,i] + 1.19*MSal_rn[j,i] + 0.0167*Watercolumn[j]                     #Speed of sound (m/s)
        A1 = (8.86 * (10**(0.78*pH - 5)))/ c                    #
        f1 = (2.8 * np.sqrt(MSal_rn[j,i]/35) )* (10**((4 - 1245)/(273+MTemp_rn[j,i])))            #unit= kHz
        alphapI= (A1*1*f1*f**2)/(f1**2+f**2)                                                                #dB/km-1
        
        #Part II: Sound absorption by MgSO4 CHECK
        A2 = 21.44 * (MSal_rn[j,i]/c) * (1 + 0.025 *MTemp_rn[j,i])                               # dB/km/kHz
        P2 = 1 - 1.37e-4*Watercolumn[j]  + 6.2e-9*Watercolumn[j] **2
        f2 = (8.17 * 10 **((8 - 1990)/(273+MTemp_rn[j,i]))) / (1+0.0018*(MSal_rn[j,i]-35))        #kHz
        alphapII= (A2*P2*f2*f**2)/(f**2+f2**2) 
        
        #Part III: Sound absorption by Pure water CHECK      
        if MTemp_rn[j,i] <= 20:
            A3 = 4.937e-4 - 2.59e-5*MTemp_rn[j,i] + 9.11e-7*MTemp_rn[j,i]**2 - 1.5e-8*MTemp_rn[j,i]**3 
        
        if MTemp_rn[j,i] > 20:
            A3 = 3.964e-4 - 1.146e-5*MTemp_rn[j,i] + 1.45e-7*MTemp_rn[j,i]**2 - 6.5e-10*MTemp_rn[j,i]**3
            
        P3 = 1.0 - 3.83e-5*Watercolumn[j]  + 4.9e-10*Watercolumn[j]**2 
            
        alphapIII= A3*P3*f**2
        
        #Total sound absorption CHECK
        SOUND_ABS = alphapI + alphapII + alphapIII      #dB/km-1       

        Sound_abs[i,j]=SOUND_ABS
    
        #Determining time varying gain = Total acoustic loss
        TVG=(20*np.log10(len(RangeADCP)-RangeADCP[j]+0.5))+(2*(SOUND_ABS/1000)*((len(RangeADCP)-RangeADCP[j]+0.5))) # Sounds_abs = dB /m
        
        Tvg[i,j]=TVG
        
        #Adding up acoustig loss to relative backscatterdata to give indication of actual backscatter value
        Backscatcol1_2[i,j]=(Backscatcol1_1[i,j]+TVG)
    
        #Next loop    
        j=j+1
        
    i=i+1
         
#Check relation between TVG, original backscatter and range normalized backscatter
# =============================================================================
# plt.plot(Tvg[1,:])
# plt.ylabel('tvg')
# plt.xlabel('depth in meters')
# 
# plt.plot(Tvg[1,:],label='tvg')
# plt.plot(Backscatcol1_1[1,:], label='Backscat original')#; plt.ylabel('Backscat original')
# plt.plot(Backscatcol1_2[1,:], label='Backscat range normalized')#; plt.ylabel('Backscat range normalized')
# plt.ylabel('Depth')
# plt.gca().invert_yaxis()
# plt.legend()
# =============================================================================









#Section 3.2.2 in thesis - CTD data processing______________________________________________________________________________________________________________________

#IMPORT Model Temp + Sal data
MTemp=matCTD['T'][5,:,:]        #Potential temperature (degrees Celsius)
MSal=matCTD['S'][5,:,:]         #Practical salinity
MDepth=matCTD['Z'][5,:]         #Depth
MTime=pd.to_datetime((matCTD['TIME'][:])-719529, unit='D').round('s')


#Section 3.2.2 in thesis - CTD data processing - Computation densitity on basis of the Sal, and Temp from the NORFJORD model
MDens=np.zeros((len(MDepth),len(MTime)),dtype='float')

for i in range(len(MDepth)):
    
    for j in range(len(MTime)):
        
        e=gsw.density.rho_t_exact(MSal[i,j],MTemp[i,j],MDepth[i])

        MDens[i,j]=e
        
        j=j+1
        
    i=i+1

#Section 3.2.2 in thesis - CTD data processing - Data alignment
MTime_1=np.arange(0,(len(MTime)-1)*48+1,1, dtype='datetime64[h]')
Mtime_2=(pd.Series(MSal[1,:],index=MTime).resample('30min').mean()).index
MSal_1=np.zeros((int(np.max(Depth))+1,len(MTime)),dtype='float')
MSal_2=np.zeros((int(np.max(Depth))+1,len(MTime_1)),dtype='float')
MTemp_1=np.zeros((int(np.max(Depth))+1,len(MTime)),dtype='float')
MTemp_2=np.zeros((int(np.max(Depth))+1,len(MTime_1)),dtype='float')
MDens_1=np.zeros((int(np.max(Depth))+1,len(MTime)),dtype='float')
MDens_2=np.zeros((int(np.max(Depth))+1,len(MTime_1)),dtype='float')

#First I am interpolating from depth in bins, to depth in meters depth
for i in range(len(MTime)):
    
    a=scipy.interpolate.interp1d(MDepth,MSal[:,i],fill_value='extrapolate')      #interpolate measurments from bins to entire depth ADCP.

    MSal_1[:,i]=a(Watercolumn)
    
    b=scipy.interpolate.interp1d(MDepth,MTemp[:,i],fill_value='extrapolate')      #interpolate measurments from bins to entire depth ADCP.

    MTemp_1[:,i]=b(Watercolumn)
    
    c=scipy.interpolate.interp1d(MDepth,MDens[:,i],fill_value='extrapolate')      #interpolate measurments from bins to entire depth ADCP.

    MDens_1[:,i]=c(Watercolumn)

    i=i+1

#Then I interpolate from the current timedelta to the preffered time delta (30 min)
for i in range(len(Watercolumn)):
    
    a=pd.Series(MSal_1[i,:],index=MTime).resample('30min').mean()

    MSal_2[i,:]=a.interpolate(method='time')

    b=pd.Series(MTemp_1[i,:],index=MTime).resample('30min').mean()

    MTemp_2[i,:]=b.interpolate(method='time')

    c=pd.Series(MDens_1[i,:],index=MTime).resample('30min').mean()

    MDens_2[i,:]=c.interpolate(method='time')

    i=i+1 
    
    
    
    
    
#Section 3.2.2 in thesis - Sediment trap data processing______________________________________________________________________________________________________________________        

#Import sediment trap data
Sedimenttrap=pd.read_csv('Bergsfjorden_Sedimenttrap.csv',sep=';')
Distancefromfarm=[-10,10,100,200,400,700,1500]
      
      
      
      
      
      
#Section 3.2.2 in thesis - Meteorological data processing______________________________________________________________________________________________________________________    
    
#Import csv
Temperature=pd.read_csv('Bergsfjorden weather_temp.csv', sep=';',error_bad_lines=False)
Wind=pd.read_csv('Bergsfjorden weather_wind (direc).csv', sep=';',error_bad_lines=False)

#Section 3.2.2 in thesis - Meteorological data processing - Data alignment -> Frouin
#Temperature
WTemptime=pd.to_datetime(Temperature.iloc[0:5111,2], format='%d.%m.%Y %H:%M')
WTempTorsvag=Temperature.iloc[0:5111,3]
WTempLangfjord=Temperature.iloc[5111:10209,3]
WTempHasvik=Temperature.iloc[10209:15320,3]
#Wind
Wwind_time=pd.to_datetime(Temperature.iloc[0:5111,2], format='%d.%m.%Y %H:%M')
WindHasvik=Wind.iloc[0:5111,4]
WinddirHasvik=Wind.iloc[0:5111,3]

#Section 3.2.2 in thesis - Meteorological data processing - Computation of solar irradiance -> Frouin

#Make datafiles
days=np.arange(1,366,1/24)                            #day number of the year
months=np.arange(1,366,31)
angle=np.zeros((2,len(days)), dtype='float')       #Angle of the sun at each day. p.11 Iqbal 1.2.2
hourangle=np.arange(180,-180,-15)                      #Angle of the sun as opposed to the 
#hourangle=np.arange(15,375,15)                      #Angle of the sun as opposed to the 
for i in range(int(len(days)/24)):
    
    angle[1,24*i:24*i+24]=hourangle# / (180/np.pi) for radians
    i=i+1

ddo=np.zeros((1,len(days)), dtype='float')            #Ratio between distance of the sun and mean of distance between sun and earth
declination= np.zeros((1,len(days)), dtype='float')   #Declination of the sun at each day. p.15 Iqbal 1.3.1
zen=np.zeros((1,len(days)), dtype='float')            #Senith angle hourly
Uv=np.zeros((1,len(days)), dtype='float')             #amount of precipitable water in the atmosphere (cm) 
I=np.zeros((1,len(days)), dtype='float')              #Solar irradiance

#Site specific parameters
latitude = 70.24# / (180/np.pi) for radians #latitude Bergsfjorden
#latitude = 40.78330492607448 / (180/np.pi) # latitude New York in radians
#declination = -8.66749475066716 / (180/np.pi)
#hourangle = 15 /  (180/np.pi)
Uo_monthly = [0.34, 0.40, 0.46, 0.42, 0.40, 0.36, 0.34, 0.31, 0.29, 0.28, 0.29, 0.31] #Iqbal p.89 Table 5.3.2 70N. Montly ozone concentration in the air
Uo_hourly = np.interp(days,months,Uo_monthly)

humidity_monthly = [0.68,0.70,0.70,0.67,0.65,0.69,0.73,0.83,0.80,0.79,0.70,0.71]# seklima.met.no Nuvsvåg weather station monthly humidity
humidity_hourly = np.interp(days,months,humidity_monthly)

Temp_monthly = [-1.3+273.15, -1.5+273.15, -1.7+273.15, 0.2+273.15, 4+273.15, 8.9+273.15, 13.1+273.15, 11.2+273.15, 8.6+273.15, 4.5+273.15, 3.2+273.15, -0.4+273.15]# seklima.met.no Nuvsvåg weather station monthly temp in Kelvin
Temp_hourly = np.interp(days,months,Temp_monthly)

#Standardized parameters
I0=  1355.49 #Iqbal p.47 (SUM I0 - lambda 4000) Monochromatic extraterrestrial irradiance between wavelength lambda1 and lambda2
a  = 0.066 #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
ai = 0.088 #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
av = 0.102 #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
ao = 0.041 #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
b  = 0.704 #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
bi = 0.456 #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
bv = 0.29  #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
bo = 0.57  #regression coefficient depends on type of arisol and spectral range, try 100% lambda 250 - 4000 + continental arosol
r= 0.4 #Iqbal p.286. Figure 9.3.2. Smooth SEA surface reflectance
V= 23 #surface visibility (km) ASSUMPTION


for i in range(len(days)):
    
    #CORRECT
    c=(2*np.pi*(days[i]-1))/365 #Iqbal p.3 formula 1.2.2
    angle[0,i]=c
    
    #CORRECT
    ddo[0,i]= 1.000110 + (0.034221*np.cos(c)) + (0.001280*np.sin(c)) + (0.000719*np.cos(2*c)) + (0.000077*np.sin(c)) #Iqbal p.3 formule 1.2.1 Ratio of actual to mean Earth- Sun separation
    
    #CORRECT
    declination[0,i] = (0.006918 - 0.399912 * np.cos(c) + 0.070257 *np.sin(c) 
                        - 0.006758* np.cos(2*c) + 0.000907 *np.sin(2*c)
                        - 0.002697 *np.cos(3*c) + 0.00148 *np.sin(3*c))* (180/np.pi) #Iqbal  p.7 formula 1.3.1 in radians ( * 180/np.pi) to convert to degrees
    
    #CORRECT
    zen[0,i] = math.acos((np.sin(np.deg2rad(declination[0,i]))*np.sin(np.deg2rad(latitude)))+(np.cos(np.deg2rad(declination[0,i]))*np.cos(np.deg2rad(latitude))*np.cos(np.deg2rad(angle[1,i]))))*(180/np.pi) #Iqbal p.15-17 formula 1.5.1 zenith angle of the sun in degrees

    #CORRECT
    Uv[0,i] = (0.493*humidity_hourly[i]*(np.exp(26.23 - 5416/Temp_hourly[i])))/Temp_hourly[i] #Iqbal p.94 formula 5.4.6 and 5.4.7. amount of precipitable water in the atmosphere (cm) 

    #CHECK THIS
    I[0,i] = ((I0 * ((ddo[0,i])**2))\
            *(np.cos(np.deg2rad(zen[0,i]))*((np.exp((-1*((a+b)/V))/np.cos(np.deg2rad(zen[0,i]))))/(1-r*((ai + bi)/V))))\
            *(np.exp(-1*av *(Uv[0,i]/np.cos(np.deg2rad(zen[0,i])))**(bv)))\
            *((np.exp(-1*ao *(Uo_hourly[i]/np.cos(np.deg2rad(zen[0,i])))**(bo)))))#Irradiance between wavelength lambda1 and lambda2.
        
    i=i+1

I[np.isnan(I)] = 0

#Section 3.2.2 in thesis - Aquaculture data processing - ______________________________________________________________________________________________________________________

#Section 3.2.2 in thesis - Aquaculture data processing - Data alignment
Aquaculture=pd.read_csv('Bergsfjorden_aquaculture.csv',sep=',')
AC_Tim=pd.to_datetime(np.unique(Aquaculture.iloc[1:,5]), format='%d.%m.%Y')
AC_Time=np.zeros((1,len(AC_Tim)),dtype='datetime64[h]')
AC_Time[0,:]=AC_Tim
AC_Time=np.sort(AC_Time)

a=np.where(Aquaculture.iloc[:,4]=='1')
Aquacultureunit1=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='2')
Aquacultureunit2=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='3')
Aquacultureunit3=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='4')
Aquacultureunit4=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='5')
Aquacultureunit5=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='6')
Aquacultureunit6=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='7')
Aquacultureunit7=Aquaculture.iloc[a]

a=np.where(Aquaculture.iloc[:,4]=='8')
Aquacultureunit8=Aquaculture.iloc[a]



#Section 3.2.2 in thesis - Aquaculture data processing - Combining net cages
Feeding=np.zeros((1,len(AC_Time[0,:])),dtype='float')

i=584
for i in range(len(AC_Time[0,:])):
    
    a=AC_Time[0,i]
    a1=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit1.iloc[:,5],format='%d.%m.%Y')))
    a2=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit2.iloc[:,5],format='%d.%m.%Y')))
    a3=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit3.iloc[:,5],format='%d.%m.%Y')))
    a4=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit4.iloc[:,5],format='%d.%m.%Y')))
    a5=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit5.iloc[:,5],format='%d.%m.%Y')))

    if not all(a1) is True : b1=0
    else: b1=float(Aquacultureunit1.iloc[int(a1[0]),7])

    if not all(a2) is True : b2=0
    else: b2=float(Aquacultureunit2.iloc[int(a2[0]),7])

    if not all(a3) is True : b3=0
    else: b3=float(Aquacultureunit3.iloc[int(a3[0]),7])

    if not all(a4) is True : b4=0
    else: b4=float(Aquacultureunit4.iloc[int(a4[0]),7])

    if not all(a5) is True : b5=0
    else: b5=float(Aquacultureunit5.iloc[int(a5[0]),7])

    Feeding[0,i]=b1+b2+b3+b4+b5

    i=i+1

plt.plot(Feeding[0,:])

Biomass=np.zeros((1,len(AC_Time[0,:])),dtype='float')

i=584
for i in range(len(AC_Time[0,:])):
    
    a=AC_Time[0,i]
    a1=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit1.iloc[:,5],format='%d.%m.%Y')))
    a2=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit2.iloc[:,5],format='%d.%m.%Y')))
    a3=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit3.iloc[:,5],format='%d.%m.%Y')))
    a4=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit4.iloc[:,5],format='%d.%m.%Y')))
    a5=np.where(np.abs(AC_Time[0,i] == pd.to_datetime(Aquacultureunit5.iloc[:,5],format='%d.%m.%Y')))

    if not all(a1) is True : b1=0
    else: b1=float(Aquacultureunit1.iloc[int(a1[0]),10])

    if not all(a2) is True : b2=0
    else: b2=float(Aquacultureunit2.iloc[int(a2[0]),10])

    if not all(a3) is True : b3=0
    else: b3=float(Aquacultureunit3.iloc[int(a3[0]),10])

    if not all(a4) is True : b4=0
    else: b4=float(Aquacultureunit4.iloc[int(a4[0]),10])

    if not all(a5) is True : b5=0
    else: b5=float(Aquacultureunit5.iloc[int(a5[0]),10])

    Biomass[0,i]=b1+b2+b3+b4+b5

    i=i+1







#Section 3.2.3 in thesis: Time series analysis__________________________________________________________________________________________________________________________

#Time series analysis - 1. interpolate all the datasets that are being analysed
a=pd.DataFrame(Backscatcol1_2)
Backscatcol1_3=a.interpolate(method='linear', axis='columns',limit_direction='both')
#plt.plot(Watercolumn,a.iloc[4000,:],'bo', Watercolumn, Backscatcol1_3.iloc[4000,:],'r')    

a=pd.DataFrame(VertvelABcol1_1)
VertvelABcol1_2=a.interpolate(method='linear', axis='columns',limit_direction='both')
#plt.plot(Watercolumn,a.iloc[4000,:],'bo', Watercolumn, VertvelABcol1_2.iloc[4000,:],'r')    

a=pd.DataFrame(Vertvelcol1_1)
Vertvelcol1_2=a.interpolate(method='linear', axis='columns',limit_direction='both')
#plt.plot(Watercolumn,a.iloc[4000,:],'bo', Watercolumn, Vertvelcol1_2.iloc[4000,:],'r')    




#Section 3.2.3 in thesis: Time series analysis - 2. FFT What patterns are visible in the data
#Mooring1 - backscat
numbersig=10

Backscatfftfreq=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
BackscatfftPSD=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

BackscatfftPSDcleanlow=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
BackscatfftPSDcleanhigh=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

Backscatffiltlow=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
Backscatffilthigh=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

Backscatindiceslow=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
Backscatindiceshigh=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

Dominantcyclesbackscat=np.zeros((numbersig,len(Backscatcol1_3.iloc[1,:])),float)

for i in range(len(Backscatcol1_3.iloc[1,:])-1):

    dt=1
    n=len(Time)
    fhat=np.fft.fft(Backscatcol1_3.iloc[:,i],n)
    BackscatfftPSD[:,i]=(1/(dt*n))*np.conj(fhat)/n
    Backscatfftfreq[:,i]=(48/(dt*n))*np.arange(n)
    L=np.arange(1,np.floor(n/2),dtype='int')
    
    a=np.argpartition(BackscatfftPSD[:len(L),i], -numbersig)[-numbersig:]   
    b=BackscatfftPSD[a,i]
    Dominantcyclesbackscat[:,i]=1/Backscatfftfreq[a,i]
    #Dominantcyclesbackscat[:,i].sort(axis=0)
    
    Backscatindiceslow[:,i]=BackscatfftPSD[:,i]<np.min(b[np.nonzero(b)]) #NEED TO VARY THIS THROUGHOUT TIME
    BackscatfftPSDcleanlow[:,i]=BackscatfftPSD[:,i]*Backscatindiceslow[:,i]  
    fhatlow=Backscatindiceslow[:,i]*fhat
    Backscatffiltlow[:,i]=np.fft.ifft(fhatlow)

    Backscatindiceshigh[:,i]=BackscatfftPSD[:,i]>np.min(b[np.nonzero(b)]) #NEED TO VARY THIS THROUGHOUT TIME 
    BackscatfftPSDcleanhigh[:,i]=BackscatfftPSD[:,i]*Backscatindiceshigh[:,i]
    fhathigh=Backscatindiceshigh[:,i]*fhat
    Backscatffilthigh[:,i]=np.fft.ifft(fhathigh)

    i=i+1

#Mooring1 - vvel
vvelfftfreq=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelfftPSD=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelfftPSDclean=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelffilt=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelindices=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
Dominantcyclesvvel=np.zeros((numbersig,len(Backscatcol1_3.iloc[1,:])),float)

for i in range(len(Backscatcol1_3.iloc[1,:])-1):

    dt=1
    n=len(Time)
    fhat=np.fft.fft(Vertvelcol1_2.iloc[:,i],n)
    vvelfftPSD[:,i]=(1/(dt*n))*np.conj(fhat)/n
    vvelfftfreq[:,i]=(48/(dt*n))*np.arange(n)
    L=np.arange(1,np.floor(n/2),dtype='int')
    
    a=np.argpartition(vvelfftPSD[:len(L),i], -numbersig)[-numbersig:]   
    b=vvelfftPSD[a,i]
    Dominantcyclesvvel[:,i]=1/vvelfftfreq[a,i]
    #Dominantcyclesvvel[:,i].sort(axis=0)
    
    vvelindices[:,i]=vvelfftPSD[:,i]>np.min(b[np.nonzero(b)]) #NEED TO VARY THIS THROUGHOUT TIME #kill everything below 0.000060
 
    vvelfftPSDclean[:,i]=vvelfftPSD[:,i]*vvelindices[:,i]
    fhat=vvelindices[:,i]*fhat
    vvelffilt[:,i]=np.fft.ifft(fhat)

    i=i+1

#Section 3.2.3 in thesis: Time series analysis - 3. low-pass filters
 
#12h
dt=1                            #Sampling interval of timeseries (halfhours)
cutoff=24                       #Cut off period (halfhours) (low-pass (lp) > 24-36h removes tides + inertial oscillations)
order=2                         #order of filter (here, a Butterworth filter use order=2 or order=4 (higher=stronger damping))
nyq=1/2/dt                      #Nyquist frequency (1/halfhours)
Wn=1/cutoff/nyq                 #filter coefficient

[b,a]=scipy.signal.butter(order,Wn,'lowpass')

Backscatcol1_lp12h=np.zeros((len(Backscatcol1_2[:,1]),len(Backscatcol1_2[1,:])),dtype='float')
VertvelABcol1_lp12h=np.zeros((len(VertvelABcol1_1[:,1]),len(VertvelABcol1_1[1,:])),dtype='float')
Vertvelcol1_lp12h=np.zeros((len(Vertvelcol1_1[:,1]),len(Vertvelcol1_1[1,:])),dtype='float')

for i in range(len(Backscatcol1_2[:,1])):
       
    Backscatcol1_lp12h[i,:]=sig.lfilter(b,a,Backscatcol1_3.iloc[i,:])
    VertvelABcol1_lp12h[i,:]=sig.lfilter(b,a,VertvelABcol1_2.iloc[i,:])
    Vertvelcol1_lp12h[i,:]=sig.lfilter(b,a,Vertvelcol1_2.iloc[i,:])

    i=i+1

#Section 3.2.3 in thesis: Time series analysis - 4. Band-pass filters

dt=1            
nyq=1/2/dt
order=2

#0.5 day
Backscatffilt05d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=0.45*48   ; low=1/lowcut/nyq     #cutoff frequency in half hours
highcut=0.55*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt05d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#1 day
Backscatffilt1d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=0.95*48   ; low=1/lowcut/nyq     #cutoff frequency in half hours
highcut=1.05*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt1d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#1-10 days
Backscatffilt110d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=1*48   ; low=1/lowcut/nyq
highcut=10*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt110d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#10-20 days
Backscatffilt1020d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=10*48   ; low=1/lowcut/nyq
highcut=20*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt1020d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#28-30 days
Backscatffilt2830d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=28*48   ; low=1/lowcut/nyq
highcut=30*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt2830d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#80-30
Backscatffilt3080d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=30*48   ; low=1/lowcut/nyq
highcut=80*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt3080d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#80-96 days
Backscatffilt8096d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=80*48   ; low=1/lowcut/nyq
highcut=96*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt8096d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1

#160-194 days
Backscatffilt164d=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

lowcut=160*48  ; low=1/lowcut/nyq
highcut=194*48  ; high=1/highcut/nyq

[b,a]=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

for i in range(len(Backscatcol1_3.iloc[1,:])):
       
    Backscatffilt164d[:,i]=sig.lfilter(b,a,Backscatcol1_3.iloc[:,i])

    i=i+1


#Section 3.2.3 in thesis: Time series analysis - 5. Model days

#Mooring 1

#Combine measured backscatter of each week into one 'weekday' do this for every week.
Weeks=np.zeros((int(len(Backscatcol1_2[:,1])/(48*7)+1),2),dtype='datetime64[h]')

for i in range(len(Weeks)):                             #Mark beginning and end of each week.

    Weeks[i,0]=np.datetime64(Time[0])+np.timedelta64(i,'W')
    Weeks[i,1]=np.datetime64(Time[0])+np.timedelta64(i+1,'W')-np.timedelta64(1,'h')
 
    i=i+1

#Sort ADCP backscatter and vertical velocity data in weeks    
Backscatweek= {}
VertvelABweek={}
Vertvelweek={}

for i in range(len(Weeks)-1):

    start=np.where(Time==Weeks[i,0])
    end=np.where(Time==Weeks[i,1])
    Backscatweek[i]=Backscatcol1_lp12h[int(start[0]):int(end[0]),:]
    VertvelABweek[i]=VertvelABcol1_lp12h[int(start[0]):int(end[0]),:]
    Vertvelweek[i]=Vertvelcol1_lp12h[int(start[0]):int(end[0]),:]
    
    i=i+1

# Make model day by averiging 9am for 6 days, 10am, etc. to have a 24 average where an everage 24h is visible.
halfhours=np.arange(0,48,1)
Backscatcol1_6dav={}
VertvelABcol1_6dav={}
Vertvelcol1_6dav={}

for j in range (len(Weeks)-1):
             
        avbackscatday=np.zeros((len(halfhours),len(Watercolumn)),dtype='float')
        avvertvelABday=np.zeros((len(halfhours),len(Watercolumn)),dtype='float')
        avvertvelday=np.zeros((len(halfhours),len(Watercolumn)),dtype='float')
   
        for i in range(len(halfhours)):
        
            if i <=22:      
                houraverage=((Backscatweek[j][i+0*48,:]+
                              Backscatweek[j][i+1*48,:]+
                              Backscatweek[j][i+2*48,:]+
                              Backscatweek[j][i+3*48,:]+
                              Backscatweek[j][i+4*48,:]+
                              Backscatweek[j][i+5*48,:]+
                              Backscatweek[j][i+6*48,:])/7)
                avbackscatday[i,:]=houraverage                
                houraverage=((VertvelABweek[j][i+0*48,:]+
                              VertvelABweek[j][i+1*48,:]+
                              VertvelABweek[j][i+2*48,:]+
                              VertvelABweek[j][i+3*48,:]+
                              VertvelABweek[j][i+4*48,:]+
                              VertvelABweek[j][i+5*48,:]+
                              VertvelABweek[j][i+6*48,:])/7)
                avvertvelABday[i,:]=houraverage
                houraverage=((Vertvelweek[j][i+0*48,:]+
                              Vertvelweek[j][i+1*48,:]+
                              Vertvelweek[j][i+2*48,:]+
                              Vertvelweek[j][i+3*48,:]+
                              Vertvelweek[j][i+4*48,:]+
                              Vertvelweek[j][i+5*48,:]+
                              Vertvelweek[j][i+6*48,:])/7)
                avvertvelday[i,:]=houraverage                
                
            else:
                houraverage=((Backscatweek[j][i+0*48,:]+
                              Backscatweek[j][i+1*48,:]+
                              Backscatweek[j][i+2*48,:]+
                              Backscatweek[j][i+3*48,:]+
                              Backscatweek[j][i+4*48,:]+
                              Backscatweek[j][i+5*48,:])/6)
                avbackscatday[i,:]=houraverage                
                houraverage=((VertvelABweek[j][i+0*48,:]+
                              VertvelABweek[j][i+1*48,:]+
                              VertvelABweek[j][i+2*48,:]+
                              VertvelABweek[j][i+3*48,:]+
                              VertvelABweek[j][i+4*48,:]+
                              VertvelABweek[j][i+5*48,:])/6)
                avvertvelABday[i,:]=houraverage
                houraverage=((Vertvelweek[j][i+0*48,:]+
                              Vertvelweek[j][i+1*48,:]+
                              Vertvelweek[j][i+2*48,:]+
                              Vertvelweek[j][i+3*48,:]+
                              Vertvelweek[j][i+4*48,:]+
                              Vertvelweek[j][i+5*48,:])/6)
                avvertvelday[i,:]=houraverage                
                
            i=i+1
            
        Backscatcol1_6dav[j]=avbackscatday
        VertvelABcol1_6dav[j]=avvertvelABday
        Vertvelcol1_6dav[j]=avvertvelday          
  
        j=j+1

#Mooring1 - backscat
numbersig=10

Backscatfftfreq=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
BackscatfftPSD=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

BackscatfftPSDcleanlow=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
BackscatfftPSDcleanhigh=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

Backscatffiltlow=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
Backscatffilthigh=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

Backscatindiceslow=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
Backscatindiceshigh=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')

Dominantcyclesbackscat=np.zeros((numbersig,len(Backscatcol1_3.iloc[1,:])),float)

for i in range(len(Backscatcol1_3.iloc[1,:])-1):

    dt=1
    n=len(Time)
    fhat=np.fft.fft(Backscatcol1_3.iloc[:,i],n)
    BackscatfftPSD[:,i]=(1/(dt*n))*np.conj(fhat)/n
    Backscatfftfreq[:,i]=(48/(dt*n))*np.arange(n)
    L=np.arange(1,np.floor(n/2),dtype='int')
    
    a=np.argpartition(BackscatfftPSD[:len(L),i], -numbersig)[-numbersig:]   
    b=BackscatfftPSD[a,i]
    Dominantcyclesbackscat[:,i]=1/Backscatfftfreq[a,i]
    #Dominantcyclesbackscat[:,i].sort(axis=0)
    
    Backscatindiceslow[:,i]=BackscatfftPSD[:,i]<np.min(b[np.nonzero(b)]) #NEED TO VARY THIS THROUGHOUT TIME
    BackscatfftPSDcleanlow[:,i]=BackscatfftPSD[:,i]*Backscatindiceslow[:,i]  
    fhatlow=Backscatindiceslow[:,i]*fhat
    Backscatffiltlow[:,i]=np.fft.ifft(fhatlow)

    Backscatindiceshigh[:,i]=BackscatfftPSD[:,i]>np.min(b[np.nonzero(b)]) #NEED TO VARY THIS THROUGHOUT TIME 
    BackscatfftPSDcleanhigh[:,i]=BackscatfftPSD[:,i]*Backscatindiceshigh[:,i]
    fhathigh=Backscatindiceshigh[:,i]*fhat
    Backscatffilthigh[:,i]=np.fft.ifft(fhathigh)

    i=i+1

#Mooring1 - vvel
vvelfftfreq=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelfftPSD=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelfftPSDclean=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelffilt=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
vvelindices=np.zeros((len(Backscatcol1_3.iloc[:,1]),len(Backscatcol1_3.iloc[1,:])),dtype='float')
Dominantcyclesvvel=np.zeros((numbersig,len(Backscatcol1_3.iloc[1,:])),float)

for i in range(len(Backscatcol1_3.iloc[1,:])-1):

    dt=1
    n=len(Time)
    fhat=np.fft.fft(Vertvelcol1_2.iloc[:,i],n)
    vvelfftPSD[:,i]=(1/(dt*n))*np.conj(fhat)/n
    vvelfftfreq[:,i]=(48/(dt*n))*np.arange(n)
    L=np.arange(1,np.floor(n/2),dtype='int')
    
    a=np.argpartition(vvelfftPSD[:len(L),i], -numbersig)[-numbersig:]   
    b=vvelfftPSD[a,i]
    Dominantcyclesvvel[:,i]=1/vvelfftfreq[a,i]
    #Dominantcyclesvvel[:,i].sort(axis=0)
    
    vvelindices[:,i]=vvelfftPSD[:,i]>np.min(b[np.nonzero(b)]) #NEED TO VARY THIS THROUGHOUT TIME #kill everything below 0.000060
 
    vvelfftPSDclean[:,i]=vvelfftPSD[:,i]*vvelindices[:,i]
    fhat=vvelindices[:,i]*fhat
    vvelffilt[:,i]=np.fft.ifft(fhat)

    i=i+1


#OUTPUT Data for plotting in 'Bergsfjorden time series analysis plots'
Bergsfjorden={      
                    'Backscatcol1':Backscatcol1_3,
                    'Backscatcol1_6dav':Backscatcol1_6dav,
                    'Backscatcol1_lp12h':Backscatcol1_lp12h,
                    'Backscatfilt<10p':Backscatffiltlow,
                    'Backscatfilt>10p':Backscatffilthigh,
                    'Backscatfilt05d':Backscatffilt05d,
                    'Backscatfilt1d':Backscatffilt1d,
                    'Backscatfilt1-10d':Backscatffilt110d,
                    'Backscatfilt10-20d':Backscatffilt1020d,
                    'Backscatfilt28-30d':Backscatffilt2830d,
                    'Backscatfilt30-80dd':Backscatffilt3080d,
                    'Backscatfilt80-96d':Backscatffilt8096d,
                    'Backscatfilt164d':Backscatffilt164d,
                    'Dominantcyclesbs':Dominantcyclesbackscat,
                    'VertvelABcol1':VertvelABcol1_2,
                    'VertvelABcol1_6dav':Vertvelcol1_6dav,
                    'VertvelABcol1_lp12h':VertvelABcol1_lp12h,
                    'Vertvelcol1':Vertvelcol1_2,
                    'Vertvelcol1_6dav':Vertvelcol1_6dav,
                    'Vertvelcol1_lp12h':Vertvelcol1_lp12h,
                    'Vertvelfilt>10p':vvelffilt,
                    'Dominantcyclesvv':Dominantcyclesvvel,
                    'Speednorth': SpeedNScol1_1,
                    'Speedeast': SpeedEWcol1_1,
                    'Chl a':Chl,
                    'O2':O2,
                    'Airsaturation':Airsaturation,
                    'Turbidity':Tur,
                    'Salinity':np.transpose(MSal_rn),
                    'Temperature':np.transpose(MTemp_rn),
                    'Density':np.transpose(MDens_rn),
                    'Depth':Watercolumn,
                    'Weeks':Weeks,
                    'Time':Time,
                    'ADCPsal':Salinity
                    } #Make dictionary of everything I want to export
pickle_out=open('Bergsfjorden clean ADCP data','wb') #Make Pickle file that stores this data named 'ADCP data Kaldfjorden clean'
pickle.dump(Bergsfjorden,pickle_out)
pickle_out.close()
